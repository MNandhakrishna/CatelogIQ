# CatelogIQ

**CatelogIQ** is a Django-based data analytics platform for real-time log analysis, intelligent text classification, chatbot and anomaly detection. It integrates **Superset** for dynamic dashboards, **Azure Data Lake** for storage, and **Databricks** for big data processing and scalable ML pipelines using **PySpark**.

Secure API access, responsive UI (Tailwind CSS & Bootstrap), and cloud-ready deployment via **Render** make CatelogIQ production-ready.

---

## Purpose

CatelogIQ simplifies log analysis and anomaly detection using real-time insights, intuitive dashboards, and cloud-scale MLâ€”backed by Databricks and Azure.

---

## Key Features

### 1. Log Stream Viewer
- Explore logs from Parquet files in real-time.
- Filter by timestamp, message content, or category.
- Pagination & scrollable views with fixed headers.
- Databricks SQL & PySpark-powered querying.

---

### 2. Chatbot Powered by LLMs
- Users can ask natural language queries related to log trends, system performance, or dataset contents.
- The chatbot is powered by LLMs and custom-trained ML models to generate insightful, context-aware responses.
- Analyze chat logs by session, user, or timestamp.
- Detect drop-offs, frequent intents, and engagement.
- Uses PySpark-based Databricks notebooks for insight generation.
- It helps users:
- Understand log patterns

---

### 3. Text Classification
- Auto-classify unstructured messages from **WhatsApp, email, and SMS**.
- Designed for IT support teams (not chatbot input).
- Routes messages to:
  - **Billing and Payments**
  - **Technical Support**
  - **IT Support**
  - **Service Outages and Maintenance**

**ML/NLP:** Databricks-based classification using prompt engineering.

---

### 4. Anomaly Detection
- Detect spikes, failures, and outliers using ML models (e.g., Isolation Forest).
- Email alerts for critical anomalies.

**Visuals:** Anomaly overlays, heatmaps, time series in Superset.

---

### 5. Interactive Visualizations
- Superset dashboards with cross-filtering, drill-downs, and auto-refresh.
- Visual types:
  - Line/Bar/Pie charts
  - Word clouds
  - Realtime sortable tables

---

## Tech Stack

| Layer       | Technology                                |
|-------------|-------------------------------------------|
| Backend     | Django, Python                            |
| Frontend    | Tailwind CSS, Bootstrap, JS, HTML         |
| Data        | Azure Data Lake, Databricks SQL           |
| Visuals     | Apache Superset                           |
| Deployment  | Render                                    |
| Email       | Gmail SMTP (App Password)                 |
| AI/ML       | NLP, prompt engineering, PySpark ML       |
| VCS         | GitHub, GitHub Desktop, Meld              |

---

## Installation

### Prerequisites

- Python 3.12+
- Git / GitHub Desktop
- Azure Data Lake account
- Databricks workspace (token + HTTP path)
- Gmail (App Password enabled)
````

### Configure `.env`

```env
AZURE_ACCOUNT_NAME=your_azure_name
AZURE_ACCOUNT_KEY=your_azure_key
AZURE_FILE_SYSTEM=your_file_system
DATABRICKS_HOST=https://adb-xxxx
DATABRICKS_TOKEN=your_token
DATABRICKS_HTTP_PATH=your_http_path
EMAIL_HOST_USER=your@gmail.com
EMAIL_HOST_PASSWORD=your_gmail_app_password
```

### Run Locally

```bash
python manage.py migrate
python manage.py runserver
```

Access at: `http://localhost:8000`

---

## Project Structure

```
CATELOGIQ  
â”œâ”€â”€ pycache  
â”œâ”€â”€ catalog  
â”‚   â”œâ”€â”€ pycache  
â”‚   â”œâ”€â”€ migrations  
â”‚   â”œâ”€â”€ templates  
â”‚   â”‚   â”œâ”€â”€ init.py  
â”‚   â”‚   â”œâ”€â”€ admin.py  
â”‚   â”‚   â”œâ”€â”€ apps.py  
â”‚   â”‚   â”œâ”€â”€ models.py  
â”‚   â”‚   â”œâ”€â”€ tests.py  
â”‚   â”‚   â”œâ”€â”€ urls.py  
â”‚   â”‚   â””â”€â”€ views.py  
â”‚   â”œâ”€â”€ CatelogIQ  
â”‚   â”‚   â”œâ”€â”€ pycache  
â”‚   â”‚   â”œâ”€â”€ init.py  
â”‚   â”‚   â”œâ”€â”€ asgi.py  
â”‚   â”‚   â”œâ”€â”€ deployment.py  
â”‚   â”‚   â”œâ”€â”€ settings.py  
â”‚   â”‚   â”œâ”€â”€ urls.py  
â”‚   â”‚   â””â”€â”€ wsgi.py  
â”‚   â””â”€â”€ staticfiles  
â”œâ”€â”€ temp  
â”œâ”€â”€ templates  
â”‚   â”œâ”€â”€ about_us.html  
â”‚   â”œâ”€â”€ analysis_options.html  
â”‚   â”œâ”€â”€ anomaly_detection.html  
â”‚   â”œâ”€â”€ chatbot.html  
â”‚   â”œâ”€â”€ contact.html  
â”‚   â”œâ”€â”€ dashboard.html  
â”‚   â”œâ”€â”€ home.html  
â”‚   â”œâ”€â”€ log_analytics.html  
â”‚   â”œâ”€â”€ stream_viewer.html  
â”‚   â””â”€â”€ text_classification.html  
â”œâ”€â”€ env  
â”œâ”€â”€ production  
â”œâ”€â”€ db.sqlite3  
â”œâ”€â”€ manage.py  
â”œâ”€â”€ requirements.txt  
â”œâ”€â”€ secret_key.py  
â”œâ”€â”€ tempCodeRunnerFile.py  
â”œâ”€â”€ test_adds.py  
â””â”€â”€ test_key.py

---

## Routes

| URL Path                  | Description                        |
| ------------------------- | ---------------------------------- |
| `/`                       | Home page                          |
| `/log-analytics/`         | Upload and monitor logs            |
| `/analysis-options/`      | Choose analysis module             |
| `/stream-viewer/`         | Explore log data                   |
| `/chatbot/`               | View chatbot analytics             |
| `/anomaly-detection/`     | Anomaly insights                   |
| `/text-classification/`   | NLP classification viewer          |
| `/dashboard/`             | Superset visual dashboard (iframe) |
| `/contact/`               | Contact form                       |
| `/about-us/`              | About page                         |
| `/debug_session/`         | Developer debugging tools          |
| `/check_pipeline_status/` | Monitor pipeline execution         |
| `/reset_pipeline_status/` | Reset DLT pipeline flags           |

---

## ğŸ” API Endpoints

| Method | Endpoint                             | Description        |
| ------ | ------------------------------------ | ------------------ |
| `GET`  | `/api/catalog/search?q=term`         | Search catalog     |
| `GET`  | `/api/catalog/filter?category=value` | Filter by category |

**Auth:**
Use header: `Authorization: Bearer <API_KEY>`

---

## Development & Deployment

### Run Tests

```bash
python manage.py test
```

### Deploy on Render

1. Connect GitHub repository
2. **Build Command:**

```bash
pip install -r requirements.txt && python manage.py migrate
```

3. **Start Command:**

```bash
gunicorn CatelogIQ.wsgi
```

4. Add environment variables in the Render dashboard.

**Tips:**

* Use **PostgreSQL** in production.
* Run `collectstatic` for static files.
* Keep `.env` and secrets safe.

---

##  Contact & Links

* **Email:** [nmyaka@quantum-i.ai](mailto:nmyaka@quantum-i.ai)
* **Website:** [quantum-i.ai](https://quantum-i.ai)
* **GitHub:** [Quantumiai](https://github.com/Quantumiai/)
* **LinkedIn:** [QuantumI AI](https://www.linkedin.com/company/quantumi-ai/)
* **YouTube:** [Quantum-I Channel](https://www.youtube.com/@Quantum-I-f9d)

---
